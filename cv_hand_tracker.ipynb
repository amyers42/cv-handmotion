{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c4bcbd-dab9-4916-9187-dae568640054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Live ASL classifier with MediaPipe hand tracking + SimpleCNN\n",
    "Requirements:\n",
    "    pip install torch torchvision opencv-python pillow mediapipe\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ff130c-484b-4ca2-92e6-d87c8fdb9af5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded ResNet checkpoint successfully.\n",
      "✅ Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64 * 56 * 56, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Load trained model\n",
    "# -----------------------------\n",
    "# 1) Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 2) Instantiate the same backbone you trained\n",
    "#    Don’t pass pretrained weights here—your checkpoint has the weights.\n",
    "model = resnet18(weights=None)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 27)  # 27 classes: A–Z, del, nothing, space\n",
    "model = model.to(device)\n",
    "\n",
    "# 3) Load your checkpoint\n",
    "ckpt = torch.load(r\"best_asl_resnet_checkpoint_5.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "print(\"✅ Loaded ResNet checkpoint successfully.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Class labels\n",
    "# -----------------------------\n",
    "idx_to_class = {\n",
    "    0:  'A',\n",
    "    1:  'B',\n",
    "    2:  'Blank',\n",
    "    3:  'C',\n",
    "    4:  'D',\n",
    "    5:  'E',\n",
    "    6:  'F',\n",
    "    7:  'G',\n",
    "    8:  'H',\n",
    "    9:  'I',\n",
    "    10: 'J',\n",
    "    11: 'K',\n",
    "    12: 'L',\n",
    "    13: 'M',\n",
    "    14: 'N',\n",
    "    15: 'O',\n",
    "    16: 'P',\n",
    "    17: 'Q',\n",
    "    18: 'R',\n",
    "    19: 'S',\n",
    "    20: 'T',\n",
    "    21: 'U',\n",
    "    22: 'V',\n",
    "    23: 'W',\n",
    "    24: 'X',\n",
    "    25: 'Y',\n",
    "    26: 'Z'\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocessing transform\n",
    "# -----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# MediaPipe hand detection\n",
    "# -----------------------------\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# -----------------------------\n",
    "# Start webcam loop\n",
    "# -----------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open webcam!\")\n",
    "\n",
    "print(\"✅ Press 'q' to quit.\")\n",
    "fps, frame_count, t_start = 0, 0, time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip if you want mirror effect\n",
    "        # frame = cv2.flip(frame, 1)\n",
    "\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)\n",
    "        label = \"No hand\"\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "            lm = results.multi_hand_landmarks[0].landmark\n",
    "\n",
    "            # get bounding box coords\n",
    "            xs = [p.x for p in lm]\n",
    "            ys = [p.y for p in lm]\n",
    "            x_min = max(int(min(xs) * w) - 20, 0)\n",
    "            x_max = min(int(max(xs) * w) + 20, w)\n",
    "            y_min = max(int(min(ys) * h) - 20, 0)\n",
    "            y_max = min(int(max(ys) * h) + 20, h)\n",
    "\n",
    "            # crop & preprocess\n",
    "            crop = frame[y_min:y_max, x_min:x_max]\n",
    "            pil_img = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "            inp = transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "            # inference\n",
    "            out = model(inp)\n",
    "            pred = torch.argmax(out, 1).item()\n",
    "            label = idx_to_class[pred]\n",
    "\n",
    "            # optional: draw hand bounding box + landmarks\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0,255,0), 2)\n",
    "            mp_draw.draw_landmarks(frame, results.multi_hand_landmarks[0], mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # FPS calculation\n",
    "        frame_count += 1\n",
    "        if frame_count == 10:\n",
    "            now = time.time()\n",
    "            fps = 10 / (now - t_start)\n",
    "            t_start = now\n",
    "            frame_count = 0\n",
    "\n",
    "        # overlay label + FPS\n",
    "        cv2.putText(frame, f\"Pred: {label}\", (10,40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0), 2)\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10,80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,0), 2)\n",
    "\n",
    "        cv2.imshow(\"ASL Live Inference (MediaPipe + CNN)\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35d26c-86ee-4fd0-9478-d947b90afeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv2025)",
   "language": "python",
   "name": "cv2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
